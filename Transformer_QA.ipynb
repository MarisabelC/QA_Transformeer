{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer QA.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNF9eV5WBKQnMvpot4nKoyz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarisabelC/QA_Transformer/blob/main/Transformer_QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnz4TSrHflEL"
      },
      "source": [
        " **Set Up**\n",
        "\n",
        " Installing Huggingface package\n",
        "\n",
        "Importing  pythorch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTy0zGYRW33s"
      },
      "source": [
        "#url: https://huggingface.co/transformers/model_doc/bert.html#bertforquestionanswering\n",
        "\n",
        "#install huggingface transformer library \n",
        "!pip install transformers\n",
        "from transformers import BertForQuestionAnswering,BertTokenizer\n",
        "\n",
        "\n",
        "#Pytorch is a Python-based scientific computing package targeted at two sets of audiences:\n",
        "# A replacement for NumPy to use the power of GPUs\n",
        "# a deep learning research platform that provides maximum flexibility and speed\n",
        "import torch "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UvMh1mKjKYa"
      },
      "source": [
        "**Load BERT pretrained Model and Tokenizer**\n",
        "\n",
        "**Encode the question and context**\n",
        "\n",
        "SQuAD - Stanford Question Answering Dataset \n",
        "\n",
        "*Huggingface Pretrained Models*\n",
        "\n",
        "https://huggingface.co/transformers/pretrained_models.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYsIBepGjgG7"
      },
      "source": [
        "#load fine-tuned Bert\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "model.to('cuda')\n",
        "\n",
        "#load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "question_list = ['What percent of New York City’s population has the Coronavirus?','What kind of testing does New York city use?','Who is New York City’s top official for disease control?']\n",
        "\n",
        "context ='One of every five New York City residents tested positive for antibodies to the coronavirus, according to preliminary results described by Gov. Andrew M. Cuomo on Thursday that suggested that the virus had spread far more widely than known. If the pattern holds, the results from random testing of 3,000 people raised the tantalizing prospect that many New Yorkers — as many as 2.7 million, the governor said — who never knew they had been infected had already encountered the virus, and survived. Mr. Cuomo also said that such wide infection might mean that the death rate was far lower than believed. While the reliability of some early antibody tests has been widely questioned, researchers in New York have worked in recent weeks to develop and validate their own antibody tests, with federal approval. State officials believe that accurate antibody testing is seen as a critical tool to help determine when and how to begin restarting the economy, and sending people back to work. \"The testing also can tell you the infection rate in the population — where it\\'s higher, where it\\'s lower — to inform you on a reopening strategy,\" Mr. Cuomo said. \"Then when you start reopening, you can watch that infection rate to see if it\\'s going up and if it\\'s going up, slow down.\" The testing in New York is among several efforts by public health officials around the country to determine how many people may have been already exposed to the virus, beyond those who have tested positive. He said that while concerns about some tests on the market were valid, the state\\'s test was reliable enough to determine immunity — and, possibly, send people back to the office.'\n",
        "\n",
        "# Apply the tokenizer to the input text, treating them as a text-pair.\n",
        "input_ids = tokenizer.encode(question_list[0], context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLBLWzybjJSk"
      },
      "source": [
        "**Tokenizer's behavior**\n",
        "\n",
        "Special Tokens:\n",
        "\n",
        "\n",
        "*   [CLS] - The classifier token which is used when doing sequence classification\n",
        "*   [UNK] – The unknown token\n",
        "*   [SEP]– The separator token\n",
        "*   [PAD]– The token used for padding\n",
        "*   [MASK]”) – The token used for masking values\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtAcd2hgi1rP"
      },
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "for token, id in zip(tokens, input_ids):\n",
        "  print('{:<20} {:>6,}'.format(token, id))\n",
        "  # If this is the [SEP] token, add ************ \n",
        "  if id == tokenizer.sep_token_id:\n",
        "      print('*****************')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snEgVhE3l7uG"
      },
      "source": [
        "**Segment embeddings**\n",
        "\n",
        "Before the word embeddings go into the BERT layers, we use segment embedding, so BERT can distinguish between question and context\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9xbkmz0m9R0"
      },
      "source": [
        "# Search the input_ids for the first instance of the `[SEP]` token.\n",
        "sep_index = input_ids.index(tokenizer.sep_token_id)\n",
        "\n",
        "# The number of tokens that are contained in question includes the [SEP].\n",
        "num_seg_question = sep_index + 1\n",
        "\n",
        "# The number of tokens that are contained in context includes the [SEP].\n",
        "num_seg_context = len(input_ids) - num_seg_question\n",
        "\n",
        "# Construct the list of 0s for the question and 1s for the context.\n",
        "segment_ids = [0]*num_seg_question + [1]*num_seg_context\n",
        "\n",
        "# There should be a segment_id for every input token.\n",
        "assert len(segment_ids) == len(input_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFHWhv_AqQBM"
      },
      "source": [
        "**Feed BERT model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMj7HrjPqPIv"
      },
      "source": [
        "# The tokens representing our input text.\n",
        "input_tensor = torch.tensor([input_ids]).to('cuda')\n",
        "#['CLS','where','did','obama'.....'sep','president']\n",
        "\n",
        "# The segment IDs to differentiate question from answer_text\n",
        "segment_tensor = torch.tensor([segment_ids]).to('cuda')\n",
        "#[0,0,0,....,1,1....]\n",
        "start_scores, end_scores = model(input_tensor,token_type_ids=segment_tensor) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8TU7I3ivjeY"
      },
      "source": [
        "**Get answer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "701uCIq-vzbL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "de7399cc-b1b8-4b2a-c86e-ab044cb9b2dc"
      },
      "source": [
        "# Find the tokens with the highest `start` and `end` scores.\n",
        "answer_start = torch.argmax(start_scores) \n",
        "answer_end = torch.argmax(end_scores)\n",
        "\n",
        "\n",
        "# Combine the tokens in the answer and print it out.\n",
        "answer = tokens[answer_start]\n",
        "\n",
        "# Select the remaining answer tokens and join them with whitespace.\n",
        "for i in range(answer_start + 1, answer_end + 1):\n",
        "    \n",
        "    # If it's a subword token, then recombine it with the previous token.\n",
        "    if tokens[i][0:2] == '##':\n",
        "        answer += tokens[i][2:]\n",
        " #non ##vio ##lent   \n",
        "    # Otherwise, add a space then the token.\n",
        "    else:\n",
        "        answer += ' ' + tokens[i]\n",
        "print(question_list[0])\n",
        "print('Answer: \"' + answer.title() + '\"')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What percent of New York City’s population has the Coronavirus?\n",
            "Answer: \"One Of Every Five\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1BMLtLi1AQy"
      },
      "source": [
        "def get_answer(question,context):\n",
        "   \n",
        "  input_ids = tokenizer.encode(question, context)\n",
        "  sep_index = input_ids.index(tokenizer.sep_token_id)\n",
        "  \n",
        "  num_seg_question = sep_index + 1\n",
        "  num_seg_context = len(input_ids) - num_seg_question\n",
        "  segment_ids = [0]*num_seg_question + [1]*num_seg_context\n",
        "  \n",
        "  input_tensor =torch.tensor([input_ids]).to('cuda')\n",
        "  segment_tensor = torch.tensor([segment_ids]).to('cuda')\n",
        "  start_scores, end_scores = model(input_tensor,token_type_ids=segment_tensor) \n",
        "  \n",
        "  answer_start = torch.argmax(start_scores)\n",
        "  answer_end = torch.argmax(end_scores)\n",
        "  answer = tokens[answer_start]\n",
        "\n",
        "  for i in range(answer_start + 1, answer_end + 1):\n",
        "      if tokens[i][0:2] == '##':\n",
        "          answer += tokens[i][2:]\n",
        "      else:\n",
        "          answer += ' ' + tokens[i]\n",
        "\n",
        "  return answer.title()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfJN1Tww2jsL"
      },
      "source": [
        "for question in question_list:\n",
        "  print(question)\n",
        "  print('answer:',get_answer(question,context)+'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR5D-4WO6zGX"
      },
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased',return_token_type_ids = True)\n",
        "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')\n",
        "\n",
        "context = \"The US has passed the peak on new coronavirus cases, President Donald Trump said and predicted that some states would reopen this month.The US has over 637,000 confirmed Covid-19 cases and over 30,826 deaths, the highest for any country in the world.\"\n",
        "question = \"What was President Donald Trump's prediction?\"\n",
        "encoding = tokenizer.encode_plus(question, context)\n",
        "\n",
        "input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "start_scores, end_scores = model(torch.tensor([input_ids]), attention_mask=torch.tensor([attention_mask]))\n",
        "\n",
        "ans_tokens = input_ids[torch.argmax(start_scores) : torch.argmax(end_scores)+1]\n",
        "answer_tokens = tokenizer.convert_ids_to_tokens(ans_tokens , skip_special_tokens=True)\n",
        "\n",
        "all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "print (\"\\nAnswer Tokens: \")\n",
        "print (answer_tokens)\n",
        "\n",
        "answer_tokens_to_string = tokenizer.convert_tokens_to_string(answer_tokens)\n",
        "\n",
        "print (\"\\nFinal Answer : \")\n",
        "print (answer_tokens_to_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1F_IU1Skqec"
      },
      "source": [
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_context(url):\n",
        "  html = urllib.request.urlopen(url).read()\n",
        "  soup = BeautifulSoup(html)\n",
        "\n",
        "\n",
        "  # kill all script and style elements\n",
        "  for script in soup([\"script\", \"style\"]):\n",
        "      script.extract()    # rip it out\n",
        "\n",
        "  # get text\n",
        "  text = soup.get_text(separator=' ')\n",
        "\n",
        "  # break into lines and remove leading and trailing space on each\n",
        "  lines = (line.strip() for line in text.splitlines())\n",
        "  # break multi-headlines into a line each\n",
        "  chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "  # drop blank lines\n",
        "  context = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "  \n",
        "  # print(context)\n",
        "  return context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3A1x_3YweiP5"
      },
      "source": [
        "#https://huggingface.co/deepset/roberta-base-squad2\n",
        "\n",
        "from transformers.pipelines import pipeline\n",
        "from transformers.modeling_auto import AutoModelForQuestionAnswering\n",
        "from transformers.tokenization_auto import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\")\n",
        "nlp_qa = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
        "# context = 'One of every five New York City residents tested positive for antibodies to the coronavirus, according to preliminary results described by Gov. Andrew M. Cuomo on Thursday that suggested that the virus had spread far more widely than known. If the pattern holds, the results from random testing of 3,000 people raised the tantalizing prospect that many New Yorkers — as many as 2.7 million, the governor said — who never knew they had been infected had already encountered the virus, and survived. Mr. Cuomo also said that such wide infection might mean that the death rate was far lower than believed. While the reliability of some early antibody tests has been widely questioned, researchers in New York have worked in recent weeks to develop and validate their own antibody tests, with federal approval. State officials believe that accurate antibody testing is seen as a critical tool to help determine when and how to begin restarting the economy, and sending people back to work. “The testing also can tell you the infection rate in the population — where it’s higher, where it’s lower — to inform you on a reopening strategy,” Mr. Cuomo said. “Then when you start reopening, you can watch that infection rate to see if it’s going up and if it’s going up, slow down.” The testing in New York is among several efforts by public health officials around the country to determine how many people may have been already exposed to the virus, beyond those who have tested positive. The results appear to conform with research from Northeastern University that indicated that the coronavirus was circulating by early February in the New York area and other major cities. In California, a study using antibody testing found rates of exposure as high as 4 percent in Santa Clara County — higher than those indicated by infection tests, though not nearly as high as found in New York. Public health officials recently disclosed that a woman in Santa Clara who died on Feb. 6 was infected with the virus. In New York City, about 21 percent tested positive for coronavirus antibodies during the state survey. The rate was about 17 percent on Long Island, nearly 12 percent in Westchester and Rockland Counties and less than 4 percent in the rest of the state. State researchers sampled blood from the approximately 3,000 people they had tested over two days, including about 1,300 in New York City, at grocery and big-box stores. The results were sent to the state’s Wadsworth facility in Albany, a respected public health lab. Dr. Howard A. Zucker, the state health commissioner, said the lab had set a high bar for determining positive results, that it had been given blanket approval to develop coronavirus tests by the Food and Drug Administration and that state officials discussed this particular antibody test with the agency. He said that while concerns about some tests on the market were valid, the state’s test was reliable enough to determine immunity — and, possibly, send people back to the office. “It is a way to say this person had the disease and they can go back into the work force,” Dr. Zucker said. “A strong test like we have can tell you that you have antibodies.” But he cautioned that the length of any such immunity remained unknown. “The amount of time, we need to see. We don’t know that yet,” he said, adding, “They will last a while.” Unlike so-called diagnostic tests, which determine whether someone is infected, often using nasal swabs, blood tests for Covid-19 antibodies are intended to reveal whether a person was previously exposed and has developed an immune response. Some tests also measure the amount of antibodies present. Hours before Mr. Cuomo’s presentation, a top health official in New York City expressed general skeptical about the utility of antibody tests — especially those on the private market — when it comes to questions of immunity and critical decisions over social distancing and reopening the economy. Dr. Demetre C. Daskalakis, the city’s top official for disease control, wrote in an email alert on Wednesday that such tests “may produce false negative or false positive results,” pointing to “significant voids” in using the science to pinpoint immunity. The alert, sent to medical providers and other subscribers, went on to warn that the consequences of relying on potentially false results may lead to “providing patients incorrect guidance on preventive interventions like physical distancing or protective equipment.”'\n",
        "\n",
        "\n",
        "def get_answer(questions, url= None, context=None):\n",
        "  if (url != None):\n",
        "    context = get_context(url)\n",
        "  for question in questions:\n",
        "    answer_map = nlp_qa(context=context, question=question)\n",
        "    print(question, ' answer: ',answer_map['answer'], '. score:', answer_map['score'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXvLk1e_zB2Y"
      },
      "source": [
        "#COVID 19\n",
        "questions =['What percent of New York City’s population has the Coronavirus?','What kind of testing does New York city use?','Who is New York City’s top official for disease control?']\n",
        "get_answer(questions, url = \"https://www.nytimes.com/2020/04/23/nyregion/coronavirus-antibodies-test-ny.html\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLBJmthL2pXX"
      },
      "source": [
        "# COVID 19\n",
        "questions =['What percent of New York City’s population has the Coronavirus?','What kind of testing does New York city use?','Who is New York City’s top official for disease control?']\n",
        "context = 'One of every five New York City residents tested positive for antibodies to the coronavirus, according to preliminary results described by Gov. Andrew M. Cuomo on Thursday that suggested that the virus had spread far more widely than known. If the pattern holds, the results from random testing of 3,000 people raised the tantalizing prospect that many New Yorkers — as many as 2.7 million, the governor said — who never knew they had been infected had already encountered the virus, and survived. Mr. Cuomo also said that such wide infection might mean that the death rate was far lower than believed. While the reliability of some early antibody tests has been widely questioned, researchers in New York have worked in recent weeks to develop and validate their own antibody tests, with federal approval. State officials believe that accurate antibody testing is seen as a critical tool to help determine when and how to begin restarting the economy, and sending people back to work. “The testing also can tell you the infection rate in the population — where it’s higher, where it’s lower — to inform you on a reopening strategy,” Mr. Cuomo said. “Then when you start reopening, you can watch that infection rate to see if it’s going up and if it’s going up, slow down.” The testing in New York is among several efforts by public health officials around the country to determine how many people may have been already exposed to the virus, beyond those who have tested positive. The results appear to conform with research from Northeastern University that indicated that the coronavirus was circulating by early February in the New York area and other major cities. In California, a study using antibody testing found rates of exposure as high as 4 percent in Santa Clara County — higher than those indicated by infection tests, though not nearly as high as found in New York. Public health officials recently disclosed that a woman in Santa Clara who died on Feb. 6 was infected with the virus. In New York City, about 21 percent tested positive for coronavirus antibodies during the state survey. The rate was about 17 percent on Long Island, nearly 12 percent in Westchester and Rockland Counties and less than 4 percent in the rest of the state. State researchers sampled blood from the approximately 3,000 people they had tested over two days, including about 1,300 in New York City, at grocery and big-box stores. The results were sent to the state’s Wadsworth facility in Albany, a respected public health lab. Dr. Howard A. Zucker, the state health commissioner, said the lab had set a high bar for determining positive results, that it had been given blanket approval to develop coronavirus tests by the Food and Drug Administration and that state officials discussed this particular antibody test with the agency. He said that while concerns about some tests on the market were valid, the state’s test was reliable enough to determine immunity — and, possibly, send people back to the office. “It is a way to say this person had the disease and they can go back into the work force,” Dr. Zucker said. “A strong test like we have can tell you that you have antibodies.” But he cautioned that the length of any such immunity remained unknown. “The amount of time, we need to see. We don’t know that yet,” he said, adding, “They will last a while.” Unlike so-called diagnostic tests, which determine whether someone is infected, often using nasal swabs, blood tests for Covid-19 antibodies are intended to reveal whether a person was previously exposed and has developed an immune response. Some tests also measure the amount of antibodies present. Hours before Mr. Cuomo’s presentation, a top health official in New York City expressed general skeptical about the utility of antibody tests — especially those on the private market — when it comes to questions of immunity and critical decisions over social distancing and reopening the economy. Dr. Demetre C. Daskalakis, the city’s top official for disease control, wrote in an email alert on Wednesday that such tests “may produce false negative or false positive results,” pointing to “significant voids” in using the science to pinpoint immunity. The alert, sent to medical providers and other subscribers, went on to warn that the consequences of relying on potentially false results may lead to “providing patients incorrect guidance on preventive interventions like physical distancing or protective equipment.”'\n",
        "get_answer(questions, context = context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Pi-Rwi7zEdl"
      },
      "source": [
        "#Obama \n",
        "questions=['Where did Obama receive the nobel peace prize?', 'How did Obama justify the wars against terror?','When is war justifiable according to Obama?']\n",
        "get_answer(questions, url ='https://www.npr.org/templates/story/story.php?storyId=121276209')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmYPsLNU1wp9"
      },
      "source": [
        "#Obama\n",
        "questions=['Where did Obama receive the nobel peace prize?', 'How did Obama justify the wars against terror?','When is war justifiable according to Obama?']\n",
        "context = 'President Obama accepted the Nobel Peace Prize during a ceremony Thursday in Norway, acknowledging the paradox of receiving the award as the U.S. is embroiled in two wars, while maintaining that instruments of war have a role in preserving peace. In his acceptance speech, Obama told Nobel Committee members and guests in Oslo that achieving peace must begin with the recognition that the use of force is sometimes morally justified. \"Make no mistake: Evil does exist in the world. A nonviolent movement could not have halted Hitler\\'s armies. Negotiations cannot convince al-Qaida\\'s leaders to lay down their arms,\" he told the crowd. It was just nine days ago that Obama announced he is sending an additional 30,000 U.S. troops to Afghanistan in an effort to step up training of Afghan security forces and root out insurgents operating on the border with Pakistan. \"I understand why war is not popular, but I also know this: The belief that peace is desirable is rarely enough to achieve it,\" he said, urging support for NATO and saying peacekeeping responsibilities shouldn\\'t be left to a few countries. The president said war is justified in cases of self-defense, when civilians are being slaughtered by their own government, or a civil war threatens to engulf an entire region. Accompanied by first lady Michelle Obama, the president struck a humble tone upon arriving in the Norwegian capital after a seven-hour flight from Washington, D.C. He acknowledged the controversy surrounding the Nobel Committee\\'s decision to honor him less than a year into his presidency, saying he knew there were others more deserving of the honor. During his speech, Obama took note of the \"giants of history\" who have been honored with the Peace Prize, including humanitarian Albert Schweitzer, civil rights leader Martin Luther King Jr. and Red Cross founder Henry Dunant. \"My accomplishments are slight\" by comparison, he said. At a news conference with Norwegian Prime Minister Jens Stoltenberg earlier in the day, Obama vowed to use the award to advance his goals for peace.'\n",
        "get_answer(questions, context= context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8v0XYgGF77C"
      },
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from spacy.lang.en import English\n",
        "\n",
        "\n",
        "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
        "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\"]\n",
        "\n",
        "def getSubsFromConjunctions(subs):\n",
        "    moreSubs = []\n",
        "    for sub in subs:\n",
        "        # rights is a generator\n",
        "        rights = list(sub.rights)\n",
        "        rightDeps = {tok.lower_ for tok in rights}\n",
        "        if \"and\" in rightDeps:\n",
        "            moreSubs.extend([tok for tok in rights if tok.dep_ in SUBJECTS or tok.pos_ == \"NOUN\"])\n",
        "            if len(moreSubs) > 0:\n",
        "                moreSubs.extend(getSubsFromConjunctions(moreSubs))\n",
        "    return moreSubs\n",
        "\n",
        "def getObjsFromConjunctions(objs):\n",
        "    moreObjs = []\n",
        "    for obj in objs:\n",
        "        # rights is a generator\n",
        "        rights = list(obj.rights)\n",
        "        rightDeps = {tok.lower_ for tok in rights}\n",
        "        if \"and\" in rightDeps:\n",
        "            moreObjs.extend([tok for tok in rights if tok.dep_ in OBJECTS or tok.pos_ == \"NOUN\"])\n",
        "            if len(moreObjs) > 0:\n",
        "                moreObjs.extend(getObjsFromConjunctions(moreObjs))\n",
        "    return moreObjs\n",
        "\n",
        "def getVerbsFromConjunctions(verbs):\n",
        "    moreVerbs = []\n",
        "    for verb in verbs:\n",
        "        rightDeps = {tok.lower_ for tok in verb.rights}\n",
        "        if \"and\" in rightDeps:\n",
        "            moreVerbs.extend([tok for tok in verb.rights if tok.pos_ == \"VERB\"])\n",
        "            if len(moreVerbs) > 0:\n",
        "                moreVerbs.extend(getVerbsFromConjunctions(moreVerbs))\n",
        "    return moreVerbs\n",
        "\n",
        "def findSubs(tok):\n",
        "    head = tok.head\n",
        "    while head.pos_ != \"VERB\" and head.pos_ != \"NOUN\" and head.head != head:\n",
        "        head = head.head\n",
        "    if head.pos_ == \"VERB\":\n",
        "        subs = [tok for tok in head.lefts if tok.dep_ == \"SUB\"]\n",
        "        if len(subs) > 0:\n",
        "            verbNegated = isNegated(head)\n",
        "            subs.extend(getSubsFromConjunctions(subs))\n",
        "            return subs, verbNegated\n",
        "        elif head.head != head:\n",
        "            return findSubs(head)\n",
        "    elif head.pos_ == \"NOUN\":\n",
        "        return [head], isNegated(tok)\n",
        "    return [], False\n",
        "\n",
        "def isNegated(tok):\n",
        "    negations = {\"no\", \"not\", \"n't\", \"never\", \"none\"}\n",
        "    for dep in list(tok.lefts) + list(tok.rights):\n",
        "        if dep.lower_ in negations:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def findSVs(tokens):\n",
        "    svs = []\n",
        "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\"]\n",
        "    for v in verbs:\n",
        "        subs, verbNegated = getAllSubs(v)\n",
        "        if len(subs) > 0:\n",
        "            for sub in subs:\n",
        "                svs.append((sub.orth_, \"!\" + v.orth_ if verbNegated else v.orth_))\n",
        "    return svs\n",
        "\n",
        "def getObjsFromPrepositions(deps):\n",
        "    objs = []\n",
        "    for dep in deps:\n",
        "        if dep.pos_ == \"ADP\" and dep.dep_ == \"prep\":\n",
        "            objs.extend([tok for tok in dep.rights if tok.dep_  in OBJECTS or (tok.pos_ == \"PRON\" and tok.lower_ == \"me\")])\n",
        "    return objs\n",
        "\n",
        "def getObjsFromAttrs(deps):\n",
        "    for dep in deps:\n",
        "        if dep.pos_ == \"NOUN\" and dep.dep_ == \"attr\":\n",
        "            verbs = [tok for tok in dep.rights if tok.pos_ == \"VERB\"]\n",
        "            if len(verbs) > 0:\n",
        "                for v in verbs:\n",
        "                    rights = list(v.rights)\n",
        "                    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
        "                    objs.extend(getObjsFromPrepositions(rights))\n",
        "                    if len(objs) > 0:\n",
        "                        return v, objs\n",
        "    return None, None\n",
        "\n",
        "def getObjFromXComp(deps):\n",
        "    for dep in deps:\n",
        "        if dep.pos_ == \"VERB\" and dep.dep_ == \"xcomp\":\n",
        "            v = dep\n",
        "            rights = list(v.rights)\n",
        "            objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
        "            objs.extend(getObjsFromPrepositions(rights))\n",
        "            if len(objs) > 0:\n",
        "                return v, objs\n",
        "    return None, None\n",
        "\n",
        "def getAllSubs(v):\n",
        "    verbNegated = isNegated(v)\n",
        "    subs = [tok for tok in v.lefts if tok.dep_ in SUBJECTS and tok.pos_ != \"DET\"]\n",
        "    if len(subs) > 0:\n",
        "        subs.extend(getSubsFromConjunctions(subs))\n",
        "    else:\n",
        "        foundSubs, verbNegated = findSubs(v)\n",
        "        subs.extend(foundSubs)\n",
        "    return subs, verbNegated\n",
        "\n",
        "def getAllObjs(v):\n",
        "    # rights is a generator\n",
        "    rights = list(v.rights)\n",
        "    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
        "    objs.extend(getObjsFromPrepositions(rights))\n",
        "\n",
        "    #potentialNewVerb, potentialNewObjs = getObjsFromAttrs(rights)\n",
        "    #if potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0:\n",
        "    #    objs.extend(potentialNewObjs)\n",
        "    #    v = potentialNewVerb\n",
        "\n",
        "    potentialNewVerb, potentialNewObjs = getObjFromXComp(rights)\n",
        "    if potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0:\n",
        "        objs.extend(potentialNewObjs)\n",
        "        v = potentialNewVerb\n",
        "    if len(objs) > 0:\n",
        "        objs.extend(getObjsFromConjunctions(objs))\n",
        "    return v, objs\n",
        "\n",
        "def findSVOs(tokens):\n",
        "\n",
        "    print(tokens)\n",
        "\n",
        "    for token in tokens:\n",
        "      print(token,' :' ,token.pos_, token.dep_)\n",
        "\n",
        "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\" and tok.dep_ != \"aux\"]\n",
        "    print(verbs)\n",
        "    for v in verbs:\n",
        "        subs, verbNegated = getAllSubs(v)\n",
        "        # hopefully there are subs, if not, don't examine this verb any longer\n",
        "        if len(subs) > 0:\n",
        "            v, objs = getAllObjs(v)\n",
        "            for sub in subs:\n",
        "                for obj in objs:\n",
        "                    objNegated = isNegated(obj)\n",
        "                    svos.append((sub.lower_, \"!\" + v.lower_ if verbNegated or objNegated else v.lower_, obj.lower_))\n",
        "    return svos\n",
        "\n",
        "def getAbuserOntoVictimSVOs(tokens):\n",
        "    maleAbuser = {'he', 'boyfriend', 'bf', 'father', 'dad', 'husband', 'brother', 'man'}\n",
        "    femaleAbuser = {'she', 'girlfriend', 'gf', 'mother', 'mom', 'wife', 'sister', 'woman'}\n",
        "    neutralAbuser = {'pastor', 'abuser', 'offender', 'ex', 'x', 'lover', 'church', 'they'}\n",
        "    victim = {'me', 'sister', 'brother', 'child', 'kid', 'baby', 'friend', 'her', 'him', 'man', 'woman'}\n",
        "\n",
        "    svos = findSVOs(tokens)\n",
        "    wnl = WordNetLemmatizer()\n",
        "    passed = []\n",
        "    for s, v, o in svos:\n",
        "        s = wnl.lemmatize(s)\n",
        "        v = \"!\" + wnl.lemmatize(v[1:], 'v') if v[0] == \"!\" else wnl.lemmatize(v, 'v')\n",
        "        o = \"!\" + wnl.lemmatize(o[1:]) if o[0] == \"!\" else wnl.lemmatize(o)\n",
        "        if s in maleAbuser.union(femaleAbuser).union(neutralAbuser) and o in victim:\n",
        "            passed.append((s, v, o))\n",
        "    return passed\n",
        "\n",
        "def printDeps(toks):\n",
        "    for tok in toks:\n",
        "        print(tok.orth_, tok.dep_, tok.pos_, tok.head.orth_, [t.orth_ for t in tok.lefts], [t.orth_ for t in tok.rights])\n",
        "\n",
        "def testSVOs():\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # tok = nlp(\"making $12 an hour? where am i going to go? i have no other financial assistance available and he certainly won't provide support.\")\n",
        "    # svos = findSVOs(tok)\n",
        "    # printDeps(tok)\n",
        "    # assert set(svos) == {('i', '!have', 'assistance'), ('he', '!provide', 'support')}\n",
        "    # print(svos)\n",
        "\n",
        "    tok = nlp(\"Who is New York City top official for disease control ?\")\n",
        "    svos = findSVOs(tok)\n",
        "    printDeps(tok)\n",
        "    assert set(svos) == {('i', '!have', 'assistance')}\n",
        "\n",
        "    print(\"-----------------------------------------------\")\n",
        "    tok = nlp(\"They ate the pizza with anchovies.\")\n",
        "    svos = findSVOs(tok)\n",
        "    printDeps(tok)\n",
        "    print(svos)\n",
        "    assert set(svos) == {('they', 'ate', 'pizza')}\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n",
        "    tok = nlp(\"I have no other financial assistance available and he certainly won't provide support.\")\n",
        "    svos = findSVOs(tok)\n",
        "    printDeps(tok)\n",
        "    print(svos)\n",
        "    assert set(svos) == {('i', '!have', 'assistance'), ('he', '!provide', 'support')}\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n",
        "    tok = nlp(\"I have no other financial assistance available, and he certainly won't provide support.\")\n",
        "    svos = findSVOs(tok)\n",
        "    printDeps(tok)\n",
        "    print(svos)\n",
        "    assert set(svos) == {('i', '!have', 'assistance'), ('he', '!provide', 'support')}\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n",
        "    tok = nlp(\"he did not kill me\")\n",
        "    svos = findSVOs(tok)\n",
        "    printDeps(tok)\n",
        "    print(svos)\n",
        "    # assert set(svos) == {('he', '!kill', 'me')}\n",
        "\n",
        "    #print(\"--------------------------------------------------\")\n",
        "    #tok = nlp(\"he is an evil man that hurt my child and sister\")\n",
        "    #svos = findSVOs(tok)\n",
        "    #printDeps(tok)\n",
        "    #print(svos)\n",
        "    #assert set(svos) == {('he', 'hurt', 'child'), ('he', 'hurt', 'sister'), ('man', 'hurt', 'child'), ('man', 'hurt', 'sister')}\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n",
        "    tok = nlp(\"he told me i would die alone with nothing but my career someday\")\n",
        "    svos = findSVOs(tok)\n",
        "    printDeps(tok)\n",
        "    print(svos)\n",
        "    assert set(svos) == {('he', 'told', 'me')}\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n",
        "    tok = nlp(\"I wanted to kill him with a hammer.\")\n",
        "    svos = findSVOs(tok)\n",
        "    printDeps(tok)\n",
        "    print(svos)\n",
        "    assert set(svos) == {('i', 'kill', 'him')}\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n",
        "    tok = nlp(\"because he hit me and also made me so angry i wanted to kill him with a hammer.\")\n",
        "    svos = findSVOs(tok)\n",
        "    printDeps(tok)\n",
        "    print(svos)\n",
        "    assert set(svos) == {('he', 'hit', 'me'), ('i', 'kill', 'him')}\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n",
        "    tok = nlp(\"he and his brother shot me\")\n",
        "    svos = findSVOs(tok)\n",
        "    printDeps(tok)\n",
        "    print(svos)\n",
        "    assert set(svos) == {('he', 'shot', 'me'), ('brother', 'shot', 'me')}\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n",
        "    tok = nlp(\"he and his brother shot me and my sister\")\n",
        "    svos = findSVOs(tok)\n",
        "    printDeps(tok)\n",
        "    print(svos)\n",
        "    assert set(svos) == {('he', 'shot', 'me'), ('he', 'shot', 'sister'), ('brother', 'shot', 'me'), ('brother', 'shot', 'sister')}\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n",
        "    tok = nlp(\"the annoying person that was my boyfriend hit me\")\n",
        "    svos = findSVOs(tok)\n",
        "    printDeps(tok)\n",
        "    print(svos)\n",
        "    assert set(svos) == {('person', 'was', 'boyfriend'), ('person', 'hit', 'me')}\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n",
        "    tok = nlp(\"the boy raced the girl who had a hat that had spots.\")\n",
        "    svos = findSVOs(tok)\n",
        "    printDeps(tok)\n",
        "    print(svos)\n",
        "    assert set(svos) == {('boy', 'raced', 'girl'), ('who', 'had', 'hat'), ('hat', 'had', 'spots')}\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n",
        "    tok = nlp(\"he spit on me\")\n",
        "    svos = findSVOs(tok)\n",
        "    printDeps(tok)\n",
        "    print(svos)\n",
        "    assert set(svos) == {('he', 'spit', 'me')}\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n",
        "    tok = nlp(\"he didn't spit on me\")\n",
        "    svos = findSVOs(tok)\n",
        "    printDeps(tok)\n",
        "    print(svos)\n",
        "    assert set(svos) == {('he', '!spit', 'me')}\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n",
        "    tok = nlp(\"the boy raced the girl who had a hat that didn't have spots.\")\n",
        "    svos = findSVOs(tok)\n",
        "    printDeps(tok)\n",
        "    print(svos)\n",
        "    assert set(svos) == {('boy', 'raced', 'girl'), ('who', 'had', 'hat'), ('hat', '!have', 'spots')}\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n",
        "    tok = nlp(\"he is a nice man that didn't hurt my child and sister\")\n",
        "    svos = findSVOs(tok)\n",
        "    printDeps(tok)\n",
        "    print(svos)\n",
        "    assert set(svos) == {('he', 'is', 'man'), ('man', '!hurt', 'child'), ('man', '!hurt', 'sister')}\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n",
        "    tok = nlp(\"he didn't spit on me and my child\")\n",
        "    svos = findSVOs(tok)\n",
        "    printDeps(tok)\n",
        "    print(svos)\n",
        "    assert set(svos) == {('he', '!spit', 'me'), ('he', '!spit', 'child')}\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n",
        "    tok = nlp(\"he beat and hurt me\")\n",
        "    svos = findSVOs(tok)\n",
        "    printDeps(tok)\n",
        "    print(svos)\n",
        "    # tok = nlp(\"he beat and hurt me\")\n",
        "\n",
        "def main():\n",
        "    testSVOs()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XcuziLQM-EZ"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# object and subject constants\n",
        "OBJECT_DEPS = {\"dobj\", \"dative\", \"attr\", \"oprd\"}\n",
        "SUBJECT_DEPS = {\"nsubj\", \"nsubjpass\", \"csubj\", \"agent\", \"expl\"}\n",
        "# tags that define wether the word is wh-\n",
        "WH_WORDS = {\"WP\", \"WP$\", \"WRB\"}\n",
        "\n",
        "# extract the subject, object and verb from the input\n",
        "def extract_svo(doc):\n",
        "    sub = []\n",
        "    at = []\n",
        "    ve = []\n",
        "    for token in doc:\n",
        "        # is this a verb?\n",
        "        if token.pos_ == \"VERB\":\n",
        "            ve.append(token.text)\n",
        "        # is this the object?\n",
        "        if token.dep_ in OBJECT_DEPS or token.head.dep_ in OBJECT_DEPS:\n",
        "            at.append(token.text)\n",
        "        # is this the subject?\n",
        "        if token.dep_ in SUBJECT_DEPS or token.head.dep_ in SUBJECT_DEPS:\n",
        "            sub.append(token.text)\n",
        "    return \" \".join(sub).strip().lower(), \" \".join(ve).strip().lower(), \" \".join(at).strip().lower()\n",
        "\n",
        "# wether the doc is a question, as well as the wh-word if any\n",
        "def is_question(doc):\n",
        "    # is the first token a verb?\n",
        "    if len(doc) > 0 and doc[0].pos_ == \"VERB\":\n",
        "        return True, \"\"\n",
        "    # go over all words\n",
        "    for token in doc:\n",
        "        # is it a wh- word?\n",
        "        if token.tag_ in WH_WORDS:\n",
        "            return True, token.text.lower()\n",
        "    return False, \"\"\n",
        "\n",
        "# gather the user input and gather the info\n",
        "while True:    \n",
        "    doc = nlp(input(\"> \"))\n",
        "    # print out the pos and deps\n",
        "    for token in doc:\n",
        "        print(\"Token {} POS: {}, dep: {}\".format(token.text, token.pos_, token.dep_))\n",
        "\n",
        "    # get the input information\n",
        "    subject, verb, attribute = extract_svo(doc)\n",
        "    question, wh_word = is_question(doc)\n",
        "    print(\"svo:, subject: {}, verb: {}, attribute: {}, question: {}, wh_word: {}\".format(subject, verb, attribute, question, wh_word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXvzrUwnSczz"
      },
      "source": [
        "!pip install textacy\n",
        "import spacy\n",
        "import textacy\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = nlp(u'What kind of testing does New York City use?')\n",
        "\n",
        "text_ext = textacy.extract.subject_verb_object_triples(text)\n",
        "\n",
        "print('text:' , text_ext)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue90JFwo4gSo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "328fa08b-e865-4925-aeef-c8be671746e6"
      },
      "source": [
        "import os\n",
        "\n",
        "%cd /content\n",
        "if not os.path.exists(\"GoogleNews-vectors-negative300.bin.gz\"):\n",
        "  !wget -P /content/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "else:\n",
        "  print(\"word2vec already downloaded\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "word2vec already downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lOyb7R58cwG"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "EMBEDDING_FILE = 'GoogleNews-vectors-negative300.bin.gz'\n",
        "\n",
        "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ_kjQkH9egS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "1315387f-1fe8-4a1e-8fb0-83f68065dd1a"
      },
      "source": [
        "import scipy\n",
        "import numpy as np\n",
        "from scipy import spatial\n",
        "\n",
        "index2word_set = set(word2vec.wv.index2word)\n",
        "\n",
        "def avg_feature_vector(sentence, model, num_features, index2word_set):\n",
        "    words = sentence.split()\n",
        "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
        "    n_words = 0\n",
        "    for word in words:\n",
        "        if word in index2word_set:\n",
        "            n_words += 1\n",
        "            feature_vec = np.add(feature_vec, model[word])\n",
        "    if (n_words > 0):\n",
        "        feature_vec = np.divide(feature_vec, n_words)\n",
        "    return feature_vec\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGkAqeDhLq_H"
      },
      "source": [
        "def matching(a, b):\n",
        "    index2word_set = set(word2vec.wv.index2word)\n",
        "    s1_afv = avg_feature_vector(a, model=word2vec, num_features=300, index2word_set=index2word_set)\n",
        "    s2_afv = avg_feature_vector(b, model=word2vec, num_features=300, index2word_set=index2word_set)\n",
        "    sim = 1 - spatial.distance.cosine(s1_afv, s2_afv)\n",
        "    return sim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vZ4_XnJ9hAh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "98ee8f4c-5136-40ce-d1aa-37381b6854b9"
      },
      "source": [
        "print(matching('The United States Flag','what other kind'))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.14754468202590942\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcwvH-OlLOZQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6b95e503-2ae9-49fa-d0a7-536495f4e0a3"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import gensim.downloader as api\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nltk.download('punkt')\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "merge_nps = nlp.create_pipe(\"merge_noun_chunks\")\n",
        "merge_ents = nlp.create_pipe(\"merge_entities\")\n",
        "merge_subtok = nlp.create_pipe(\"merge_subtokens\")\n",
        "nlp.add_pipe(merge_nps)\n",
        "nlp.add_pipe(merge_ents)\n",
        "nlp.add_pipe(merge_subtok)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6TERObfLdw_"
      },
      "source": [
        "import math\n",
        "\n",
        "def returnresult(text, question, numofanswers):\n",
        "\n",
        "    question = question.lower()\n",
        "    text = text.lower()\n",
        "    quotelesstext = text.replace('“', \"\").replace('”',\"\")\n",
        "\n",
        "    questiondoc = nlp(question)\n",
        "    sen_map = {}\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.string.strip() for sent in doc.sents]\n",
        "    \n",
        "    scores ={}\n",
        "    for sentence in sentences:\n",
        "        scores[sentence] = matching(question,sentence)\n",
        "    sort_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    if sort_scores[0][1] <= 0.5 or math.isnan(sort_scores[0][1]) :\n",
        "        return ['there was no match']\n",
        "    else:\n",
        "      return sort_scores\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajdtIH2UMRPJ"
      },
      "source": [
        "import json\n",
        "\n",
        "%cd /content\n",
        "\n",
        "question_list_file_path = \"question_list.json\"\n",
        "\n",
        "with open(question_list_file_path, 'r',  encoding=\"utf8\") as file:\n",
        "\n",
        "    data = file.read()\n",
        "\n",
        "question_list = json.loads(data)\n",
        "\n",
        "correctAnswer = 0\n",
        "\n",
        "for q in question_list:\n",
        "    print(\"************************************************************\")\n",
        "    print(\"Testing \", q['name'])\n",
        "\n",
        "    text_file_path =  q['file']\n",
        "    with open(text_file_path, 'r', encoding=\"utf8\") as file:\n",
        "        text_data = file.read()\n",
        "\n",
        "    res = returnresult(text_data,  q['question'], 3)\n",
        "    print(q['question'])\n",
        "    \n",
        "    \n",
        "    for answer in res[:3]: \n",
        "        if q['Answer'].lower() == answer.lower():\n",
        "            correctAnswer+= 1\n",
        "            print('correct answer')\n",
        "            break\n",
        "            \n",
        "    print('correct answer: ' ,q['Answer'].lower() )    \n",
        "    print(res)\n",
        "    print()\n",
        "\n",
        "print('accuracy: ' , correctAnswer / len(question_list), ' correct answer: ' , correctAnswer)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}